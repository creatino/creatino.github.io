{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUUTDHqcYyFp+KNkp0FG2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/creatino/creatino.github.io/blob/master/Stock_Portfolio_v1_02022025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugg9I4Q_x_SF",
        "outputId": "675102ab-2daa-4cf8-838c-2f5488b073c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import calendar\n",
        "import os\n",
        "\n",
        "# Define the path to the Excel file containing stock data\n",
        "file_path = '../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/Top_Stocks.xlsx'\n",
        "\n",
        "# Read the Excel file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_excel(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    exit()  # Stop execution if the file is not found\n",
        "\n",
        "# Extract the stock symbols from the DataFrame\n",
        "stock_symbols = df['Symbol'].tolist()\n",
        "\n",
        "# Print the extracted stock symbols\n",
        "print(\"Extracted Stock Symbols:\", stock_symbols)\n",
        "\n",
        "\n",
        "# Define a function to calculate stock growth data\n",
        "def calculate_percentage_growth(stock_symbols):\n",
        "    # Define a dictionary containing different time periods\n",
        "    periods = {\n",
        "        '5 Days': 5,\n",
        "        '10 Days': 10,\n",
        "        '1 Month': 30,\n",
        "        '3 Months': 90,\n",
        "        '6 Months': 180\n",
        "    }\n",
        "\n",
        "    # Initialize a list to store growth data\n",
        "    growth_data = []\n",
        "\n",
        "    # Iterate over each stock symbol\n",
        "    for symbol in stock_symbols:\n",
        "        # Try downloading historical stock data using yfinance\n",
        "        try:\n",
        "            data = yf.download(symbol, period=\"2y\")['Close']\n",
        "            # Get the current date\n",
        "            today = datetime.date.today()\n",
        "\n",
        "            # Initialize an empty dictionary to store growth data for the current symbol\n",
        "            growth = {}\n",
        "\n",
        "            # Iterate over each time period\n",
        "            for period_name, period_days in periods.items():\n",
        "                # Calculate the past date based on the period\n",
        "                past_date = today - datetime.timedelta(days=period_days)\n",
        "\n",
        "                # Adjust the past date to the previous weekday if it falls on a weekend\n",
        "                while past_date.weekday() in (calendar.SATURDAY, calendar.SUNDAY):\n",
        "                    past_date -= datetime.timedelta(days=1)\n",
        "\n",
        "                # Filter the data to include only prices from the past date up to today\n",
        "                data_filtered = data[data.index.date >= past_date]\n",
        "\n",
        "                # Check if there are enough data points to calculate growth\n",
        "                if len(data_filtered) >= 2:\n",
        "                    # Calculate the percentage change between the current and past prices\n",
        "                    current_price = data_filtered.iloc[-1]\n",
        "                    past_price = data_filtered.iloc[0]\n",
        "                    percentage_change = ((current_price - past_price) / past_price) * 100\n",
        "\n",
        "                    # Store the percentage change in the growth dictionary\n",
        "                    growth[period_name] = percentage_change\n",
        "                else:\n",
        "                    # If there aren't enough data points, set the growth percentage to None\n",
        "                    growth[period_name] = None\n",
        "\n",
        "            # Append the growth dictionary for the current symbol to the growth_data list\n",
        "            growth_data.append(growth)\n",
        "\n",
        "        # Catch any errors that occur during the data download or calculation\n",
        "        except Exception as e:\n",
        "            # Print an error message indicating the symbol and the error that occurred\n",
        "            print(f\"Error calculating growth for {symbol}: {e}\")\n",
        "            growth_data.append({period_name: None for period_name in periods})  # Append None values for all periods\n",
        "\n",
        "    # Create a DataFrame from the growth data using the stock symbols as the index\n",
        "    df_growth = pd.DataFrame(growth_data, index=stock_symbols)\n",
        "\n",
        "    # Return the DataFrame containing the growth data\n",
        "    return df_growth\n",
        "\n",
        "\n",
        "# Call the function to calculate and assign the result to df_growth\n",
        "df_growth = calculate_percentage_growth(stock_symbols)\n",
        "\n",
        "# Ensure the index is unique\n",
        "df_growth.index = df_growth.index.astype(str) + '_' + df_growth.groupby(level=0).cumcount().astype(str)\n",
        "\n",
        "# Create an empty dictionary to store the stock data\n",
        "stock_data = {}\n",
        "\n",
        "# Iterate over each stock symbol\n",
        "for symbol in stock_symbols:\n",
        "    # Try downloading stock data using yfinance\n",
        "    try:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        info = ticker.info\n",
        "\n",
        "        # Extract relevant data points from the info dictionary\n",
        "        stock_data[symbol] = {\n",
        "            \"Current Stock Price\": info.get(\"currentPrice\"),\n",
        "            \"Market Capitalization\": info.get(\"marketCap\"),\n",
        "            \"Trailing P/E\": info.get(\"trailingPE\"),\n",
        "            \"Forward P/E\": info.get(\"forwardPE\"),\n",
        "            \"Price-to-Book Ratio\": info.get(\"priceToBook\"),\n",
        "            \"Price-to-Sales Ratio\": info.get(\"priceToSalesTrailing12Months\"),\n",
        "            \"52 Week High\": info.get(\"fiftyTwoWeekHigh\"),\n",
        "            \"52 Week Low\": info.get(\"fiftyTwoWeekLow\"),\n",
        "            \"Dividend Yield\": info.get(\"dividendYield\"),\n",
        "\n",
        "            # Profitability\n",
        "            \"ROA\": info.get(\"returnOnAssets\"),\n",
        "            \"Gross Profit Margin\": info.get(\"grossMargins\"),\n",
        "            \"EPS\": info.get(\"trailingEps\"),\n",
        "            \"Profit Margin\": info.get(\"profitMargins\"),\n",
        "            \"ROE\": info.get(\"returnOnEquity\"),\n",
        "\n",
        "            # Financial Health\n",
        "            \"Debt-to-Equity Ratio\": info.get(\"debtToEquity\"),\n",
        "            \"Current Ratio\": info.get(\"currentRatio\"),\n",
        "            \"Interest Coverage Ratio\": info.get(\"interestCoverage\"),\n",
        "            \"Debt-to-Asset Ratio\": info.get(\"totalDebt\") / info.get(\"totalAssets\") if info.get(\"totalDebt\") and info.get(\n",
        "                \"totalAssets\") else None,\n",
        "            \"Cash Flow\": info.get(\"operatingCashflow\"),\n",
        "\n",
        "            # Valuation\n",
        "            \"PEG Ratio\": info.get(\"pegRatio\"),\n",
        "\n",
        "            # Growth\n",
        "            \"Revenue Growth\": info.get(\"revenueGrowth\"),\n",
        "            \"Earnings Growth Rate\": info.get(\"earningsGrowth\"),\n",
        "\n",
        "            # Analyst Ratings\n",
        "            \"Analyst Recommendation\": info.get(\"recommendationKey\"),\n",
        "            \"Number of Analyst Opinions\": info.get(\"numberOfAnalystOpinions\"),\n",
        "            \"Target Low Price\": info.get(\"targetLowPrice\"),\n",
        "            \"Target High Price\": info.get(\"targetHighPrice\"),\n",
        "            \"Target Mean Price\": info.get(\"targetMeanPrice\"),\n",
        "            \"Target Median Price\": info.get(\"targetMedianPrice\"),\n",
        "\n",
        "            # Short Interest\n",
        "            \"Short Interest\": info.get(\"sharesShort\"),\n",
        "            \"Short Ratio\": info.get(\"shortRatio\"),\n",
        "            \"Short Percent of Float\": info.get(\"shortPercentOfFloat\"),\n",
        "\n",
        "            # Institutional Ownership\n",
        "            \"Held Percent Institutions\": info.get(\"heldPercentInstitutions\"),\n",
        "            \"Held Percent Insiders\": info.get(\"heldPercentInsiders\"),\n",
        "\n",
        "            # Trading Volume\n",
        "            \"Average Volume\": info.get(\"averageVolume\"),\n",
        "            \"Volume\": info.get(\"volume\"),\n",
        "        }\n",
        "    # Catch any errors that occur during the data download\n",
        "    except Exception as e:\n",
        "        # Print an error message indicating the symbol and the error that occurred\n",
        "        print(f\"Error fetching data for {symbol}: {e}\")\n",
        "        # Set the data for the symbol to 'Data not available' if there was an error\n",
        "        stock_data[symbol] = \"Data not available\"\n",
        "\n",
        "# Convert the stock data dictionary to a DataFrame\n",
        "stock_df = pd.DataFrame(stock_data).transpose()\n",
        "\n",
        "# Reset the index of the DataFrame to have 'Symbol' as a column\n",
        "stock_df.reset_index(inplace=True)\n",
        "\n",
        "# Rename the index column to 'Symbol' for clarity\n",
        "stock_df.rename(columns={'index': 'Symbol'}, inplace=True)\n",
        "\n",
        "# Define the output path to the output files\n",
        "output_file_path = '../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/'\n",
        "\n",
        "# Construct the output file paths\n",
        "stock_fundamentals_path = os.path.join(os.path.dirname(output_file_path), 'stock_fundamentals.xlsx')\n",
        "stock_growth_path = os.path.join(os.path.dirname(output_file_path), 'stock_growth.xlsx')\n",
        "\n",
        "# Write the DataFrames to separate Excel files\n",
        "stock_df.to_excel(stock_fundamentals_path, index=False)\n",
        "df_growth.to_excel(stock_growth_path, index=False)\n",
        "\n",
        "# Print success messages\n",
        "print(f\"Stock fundamental data saved to: {stock_fundamentals_path}\")\n",
        "print(f\"Stock growth data saved to: {stock_growth_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJPF3Mn7W0uW",
        "outputId": "9aeaf641-4a4f-4f2f-abb7-135057b55f10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Stock Symbols: ['SEZL', 'APP', 'RDDT', 'MSTR', 'PLTR', 'CVNA', 'LB', 'ALAB', 'GGAL', 'VST', 'PRAX', 'CRDO', 'BMA', 'IONQ', 'CLS', 'TLN', 'FTAI', 'EAT', 'AGX', 'HOOD', 'USLM', 'TECX', 'SMTC', 'VERA', 'TARS', 'NVDA', 'SFM', 'LOAR', 'CAVA', 'SE', 'IESC', 'NTRA', 'POWL', 'GEV', 'NGVC', 'YPF', 'DXPE', 'QFIN', 'VITL']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock fundamental data saved to: ../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/stock_fundamentals.xlsx\n",
            "Stock growth data saved to: ../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/stock_growth.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas yfinance openpyxl GoogleNews textblob nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpxqBqCKYosr",
        "outputId": "19807c58-ed64-4820-b35a-cc0b8415b5c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: GoogleNews in /usr/local/lib/python3.11/dist-packages (1.6.15)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.11/dist-packages (from GoogleNews) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2024.12.14)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from GoogleNews import GoogleNews\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def get_news_and_analyze_sentiment(stock_symbols):\n",
        "    \"\"\"\n",
        "    Fetches news articles, analyst opinions (if available), performs sentiment analysis,\n",
        "    and returns a DataFrame with the results.\n",
        "\n",
        "    Args:\n",
        "        stock_symbols (list): A list of stock symbols.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame with sentiment analysis results.\n",
        "    \"\"\"\n",
        "\n",
        "    sentiment_data = []\n",
        "\n",
        "    for symbol in stock_symbols:\n",
        "        print(f\"Processing {symbol}...\")\n",
        "\n",
        "        # Fetch News\n",
        "        googlenews = GoogleNews()\n",
        "        googlenews.search(symbol)\n",
        "        news_results = googlenews.results(sort=True)\n",
        "\n",
        "        # Fetch Analyst Opinion (using yfinance as a proxy - limited data)\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        try:\n",
        "            analyst_recommendation = ticker.info.get(\"recommendationKey\")\n",
        "        except:\n",
        "            analyst_recommendation = \"Not Available\"\n",
        "\n",
        "        # Sentiment Analysis\n",
        "        news_sentiment_scores = []\n",
        "\n",
        "        print(f\"  News clips for {symbol}:\")  # Print header for news clips\n",
        "\n",
        "        for article in news_results:\n",
        "            try:\n",
        "                analysis = TextBlob(article['desc'])\n",
        "                news_sentiment_scores.append(analysis.sentiment.polarity)\n",
        "                print(f\"    - {article['title']}: {article['desc'][:100]}...\")  # Print title and snippet of description\n",
        "            except Exception as e:\n",
        "                print(f\"    - Error analyzing sentiment for article: {article['title']}, Error: {e}\")\n",
        "\n",
        "        # Calculate Average Sentiment\n",
        "        avg_news_sentiment = sum(news_sentiment_scores) / len(news_sentiment_scores) if news_sentiment_scores else 0\n",
        "\n",
        "        # Classify Sentiment\n",
        "        news_sentiment_label = \"Positive\" if avg_news_sentiment > 0.1 else \"Negative\" if avg_news_sentiment < -0.1 else \"Neutral\"\n",
        "\n",
        "        #using NLTK for sentiment analysis\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "        # Perform sentiment analysis on news articles\n",
        "        news_sentiment_nltk = []\n",
        "\n",
        "        for article in news_results:\n",
        "            try:\n",
        "                scores = analyzer.polarity_scores(article['desc'])\n",
        "                news_sentiment_nltk.append(scores)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing sentiment for article: {article['title']}, Error: {e}\")\n",
        "\n",
        "        # Calculate average sentiment scores\n",
        "        avg_news_compound = sum(score['compound'] for score in news_sentiment_nltk) / len(news_sentiment_nltk) if news_sentiment_nltk else 0\n",
        "\n",
        "        # Classify sentiment based on compound score\n",
        "        if avg_news_compound >= 0.05:\n",
        "            news_sentiment_label_nltk = \"Positive\"\n",
        "        elif avg_news_compound <= -0.05:\n",
        "            news_sentiment_label_nltk = \"Negative\"\n",
        "        else:\n",
        "            news_sentiment_label_nltk = \"Neutral\"\n",
        "\n",
        "        sentiment_data.append({\n",
        "            \"Symbol\": symbol,\n",
        "            \"Analyst Recommendation\": analyst_recommendation,\n",
        "            \"Average News Sentiment\": avg_news_sentiment,\n",
        "            \"News Sentiment Label\": news_sentiment_label,\n",
        "            \"Average News Sentiment NLTK\": avg_news_compound,\n",
        "            \"News Sentiment Label NLTK\" : news_sentiment_label_nltk\n",
        "        })\n",
        "\n",
        "        googlenews.clear()\n",
        "\n",
        "    return pd.DataFrame(sentiment_data)\n",
        "\n",
        "def save_sentiment_to_excel(sentiment_df, output_path):\n",
        "    \"\"\"\n",
        "    Saves the sentiment analysis DataFrame to an Excel file.\n",
        "\n",
        "    Args:\n",
        "        sentiment_df (pandas.DataFrame): The DataFrame with sentiment data.\n",
        "        output_path (str): The path to save the Excel file.\n",
        "    \"\"\"\n",
        "    sentiment_file_path = os.path.join(output_path, 'sentiment.xlsx')\n",
        "    sentiment_df.to_excel(sentiment_file_path, index=False)\n",
        "    print(f\"Sentiment analysis data saved to: {sentiment_file_path}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to perform sentiment analysis on stocks.\n",
        "    \"\"\"\n",
        "    # Define the path to the Excel file containing stock data\n",
        "    file_path = '../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/Top_Stocks.xlsx'\n",
        "\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        exit()  # Stop execution if the file is not found\n",
        "\n",
        "    # Extract the stock symbols from the DataFrame\n",
        "    stock_symbols = df['Symbol'].tolist()\n",
        "\n",
        "    # Define the output path for sentiment analysis results\n",
        "    output_file_path = '../content/gdrive/My Drive/Colab_Notebooks/StockAnalysis/'\n",
        "\n",
        "    # --- Sentiment Analysis ---\n",
        "    sentiment_df = get_news_and_analyze_sentiment(stock_symbols)\n",
        "    save_sentiment_to_excel(sentiment_df, output_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "nbl7yOG5cC1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is a copy but now prints the hyperlinks to the source of the news articles"
      ],
      "metadata": {
        "id": "XwGXvlCSfjYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from GoogleNews import GoogleNews\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Mount Google Drive (if running in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = '/content/drive/My Drive/Colab_Notebooks/StockAnalysis/'  # Base path for Google Drive\n",
        "except:\n",
        "    base_path = ''  # Local execution, set base path accordingly\n",
        "\n",
        "\n",
        "def get_news_and_analyze_sentiment(stock_symbols):\n",
        "    \"\"\"\n",
        "    Fetches news articles, analyst opinions (if available), performs sentiment analysis,\n",
        "    and returns a DataFrame with the results.\n",
        "\n",
        "    Args:\n",
        "        stock_symbols (list): A list of stock symbols.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame with sentiment analysis results.\n",
        "    \"\"\"\n",
        "\n",
        "    sentiment_data = []\n",
        "\n",
        "    for symbol in stock_symbols:\n",
        "        print(f\"Processing {symbol}...\")\n",
        "\n",
        "        # Fetch News\n",
        "        googlenews = GoogleNews()\n",
        "        googlenews.search(symbol)\n",
        "        news_results = googlenews.results(sort=True)\n",
        "\n",
        "        # Fetch Analyst Opinion (using yfinance as a proxy - limited data)\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        try:\n",
        "            analyst_recommendation = ticker.info.get(\"recommendationKey\")\n",
        "        except:\n",
        "            analyst_recommendation = \"Not Available\"\n",
        "\n",
        "        # Sentiment Analysis and News Clip Printing\n",
        "        news_sentiment_scores = []\n",
        "\n",
        "        print(f\"  News clips for {symbol}:\")\n",
        "\n",
        "        for article in news_results:\n",
        "            try:\n",
        "                analysis = TextBlob(article['desc'])\n",
        "                news_sentiment_scores.append(analysis.sentiment.polarity)\n",
        "                # Print title, snippet of description, and link\n",
        "                print(f\"    - {article['title']}: {article['desc'][:100]}... Link: {article['link']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    - Error analyzing sentiment for article: {article['title']}, Error: {e}\")\n",
        "\n",
        "        # Calculate Average Sentiment\n",
        "        avg_news_sentiment = sum(news_sentiment_scores) / len(news_sentiment_scores) if news_sentiment_scores else 0\n",
        "\n",
        "        # Classify Sentiment\n",
        "        news_sentiment_label = \"Positive\" if avg_news_sentiment > 0.1 else \"Negative\" if avg_news_sentiment < -0.1 else \"Neutral\"\n",
        "\n",
        "        #using NLTK for sentiment analysis\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "        # Perform sentiment analysis on news articles\n",
        "        news_sentiment_nltk = []\n",
        "\n",
        "        for article in news_results:\n",
        "            try:\n",
        "                scores = analyzer.polarity_scores(article['desc'])\n",
        "                news_sentiment_nltk.append(scores)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing sentiment for article: {article['title']}, Error: {e}\")\n",
        "\n",
        "        # Calculate average sentiment scores\n",
        "        avg_news_compound = sum(score['compound'] for score in news_sentiment_nltk) / len(news_sentiment_nltk) if news_sentiment_nltk else 0\n",
        "\n",
        "        # Classify sentiment based on compound score\n",
        "        if avg_news_compound >= 0.05:\n",
        "            news_sentiment_label_nltk = \"Positive\"\n",
        "        elif avg_news_compound <= -0.05:\n",
        "            news_sentiment_label_nltk = \"Negative\"\n",
        "        else:\n",
        "            news_sentiment_label_nltk = \"Neutral\"\n",
        "\n",
        "        sentiment_data.append({\n",
        "            \"Symbol\": symbol,\n",
        "            \"Analyst Recommendation\": analyst_recommendation,\n",
        "            \"Average News Sentiment\": avg_news_sentiment,\n",
        "            \"News Sentiment Label\": news_sentiment_label,\n",
        "            \"Average News Sentiment NLTK\": avg_news_compound,\n",
        "            \"News Sentiment Label NLTK\": news_sentiment_label_nltk\n",
        "        })\n",
        "\n",
        "        googlenews.clear()\n",
        "\n",
        "    return pd.DataFrame(sentiment_data)\n",
        "\n",
        "def save_sentiment_to_excel(sentiment_df, output_path):\n",
        "    \"\"\"\n",
        "    Saves the sentiment analysis DataFrame to an Excel file.\n",
        "    \"\"\"\n",
        "    sentiment_file_path = os.path.join(output_path, 'sentiment.xlsx')\n",
        "    try:\n",
        "        sentiment_df.to_excel(sentiment_file_path, index=False)\n",
        "        print(f\"Sentiment analysis data saved to: {sentiment_file_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving sentiment data to Excel: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to perform sentiment analysis on stocks.\n",
        "    \"\"\"\n",
        "    # Define the path to the Excel file containing stock data (relative to base_path)\n",
        "    file_path = os.path.join(base_path, 'Top_Stocks.xlsx')\n",
        "\n",
        "    # Read the Excel file\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        exit()\n",
        "\n",
        "    # Extract stock symbols\n",
        "    stock_symbols = df['Symbol'].tolist()\n",
        "\n",
        "    # Define the output path for sentiment analysis results (same as base_path)\n",
        "    output_file_path = base_path\n",
        "\n",
        "    # --- Sentiment Analysis ---\n",
        "    sentiment_df = get_news_and_analyze_sentiment(stock_symbols)\n",
        "    save_sentiment_to_excel(sentiment_df, output_file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdyWnSuFfVii",
        "outputId": "727d9dff-8d28-4d3d-d6ef-fc659b376efa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Processing SEZL...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for SEZL:\n",
            "Processing APP...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for APP:\n",
            "Processing RDDT...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for RDDT:\n",
            "Processing MSTR...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for MSTR:\n",
            "Processing PLTR...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for PLTR:\n",
            "Processing CVNA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for CVNA:\n",
            "Processing LB...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for LB:\n",
            "Processing ALAB...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for ALAB:\n",
            "Processing GGAL...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for GGAL:\n",
            "Processing VST...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for VST:\n",
            "Processing PRAX...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for PRAX:\n",
            "Processing CRDO...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for CRDO:\n",
            "Processing BMA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for BMA:\n",
            "Processing IONQ...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for IONQ:\n",
            "Processing CLS...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for CLS:\n",
            "Processing TLN...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for TLN:\n",
            "Processing FTAI...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for FTAI:\n",
            "Processing EAT...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for EAT:\n",
            "Processing AGX...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for AGX:\n",
            "Processing HOOD...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for HOOD:\n",
            "Processing USLM...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for USLM:\n",
            "Processing TECX...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for TECX:\n",
            "Processing SMTC...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for SMTC:\n",
            "Processing VERA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for VERA:\n",
            "Processing TARS...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for TARS:\n",
            "Processing NVDA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for NVDA:\n",
            "Processing SFM...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for SFM:\n",
            "Processing LOAR...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for LOAR:\n",
            "Processing CAVA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for CAVA:\n",
            "Processing SE...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for SE:\n",
            "Processing IESC...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for IESC:\n",
            "Processing NTRA...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for NTRA:\n",
            "Processing POWL...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for POWL:\n",
            "Processing GEV...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for GEV:\n",
            "Processing NGVC...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for NGVC:\n",
            "Processing YPF...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for YPF:\n",
            "Processing DXPE...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for DXPE:\n",
            "Processing QFIN...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for QFIN:\n",
            "Processing VITL...\n",
            "HTTP Error 429: Too Many Requests\n",
            "  News clips for VITL:\n",
            "Sentiment analysis data saved to: /content/drive/My Drive/Colab_Notebooks/StockAnalysis/sentiment.xlsx\n"
          ]
        }
      ]
    }
  ]
}